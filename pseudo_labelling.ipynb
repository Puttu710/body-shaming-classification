{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/viviek/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/viviek/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/viviek/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/viviek/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def normalize_text(words):\n",
    "    words = to_lowercase(words)\n",
    "    return words\n",
    "\n",
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def text_prepare(text):\n",
    "    text = ' '.join([x for x in normalize_text(tokenize(text))])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and preprocessing 2k labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>its holy everyone up is quiet palmdale bbw ove...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>may you all have a peaceful tiredbuthappy deca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>costco new york is my a tastes great and paire...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>each time you shoot a zero you have to give me...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>facebook strikes again – i liked a few pages o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2154</th>\n",
       "      <td>a comment from my tutor about the recent �no m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>doing my christmas shopping i noticed a perfec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>and ive said im a feminist before but people j...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>god rescue is the best thing you could ever as...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>a boy thought it was funny to constantly tease...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2159 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     its holy everyone up is quiet palmdale bbw ove...      0\n",
       "1     may you all have a peaceful tiredbuthappy deca...      0\n",
       "2     costco new york is my a tastes great and paire...      0\n",
       "3     each time you shoot a zero you have to give me...      0\n",
       "4     facebook strikes again – i liked a few pages o...      1\n",
       "...                                                 ...    ...\n",
       "2154  a comment from my tutor about the recent �no m...      1\n",
       "2155  doing my christmas shopping i noticed a perfec...      0\n",
       "2156  and ive said im a feminist before but people j...      0\n",
       "2157  god rescue is the best thing you could ever as...      0\n",
       "2158  a boy thought it was funny to constantly tease...      1\n",
       "\n",
       "[2159 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('sp+Ip+sn+In.csv')\n",
    "df['text'] = [text_prepare(x) for x in df['text']]\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['label'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = [entry.lower() for entry in df['text']]\n",
    "df['text']= [word_tokenize(entry) for entry in df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['holy', 'everyone', 'quiet', 'palmdale', 'bbw...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['may', 'peaceful', 'tiredbuthappy', 'decathlo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['costco', 'new', 'york', 'taste', 'great', 'p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['time', 'shoot', 'zero', 'give', 'kiss']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['facebook', 'strike', 'like', 'page', 'compan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  ['holy', 'everyone', 'quiet', 'palmdale', 'bbw...      0\n",
       "1  ['may', 'peaceful', 'tiredbuthappy', 'decathlo...      0\n",
       "2  ['costco', 'new', 'york', 'taste', 'great', 'p...      0\n",
       "3          ['time', 'shoot', 'zero', 'give', 'kiss']      0\n",
       "4  ['facebook', 'strike', 'like', 'page', 'compan...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_k_data  = df\n",
    "for index,entry in enumerate(two_k_data['text']):\n",
    "    Final_words = []\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(entry):\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    two_k_data.loc[index,'text'] = str(Final_words)\n",
    "two_k_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(two_k_data['text'],two_k_data['label'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.tolist()\n",
    "y_train=y_train.tolist()\n",
    "X_test=X_test.tolist()\n",
    "y_test=y_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_transformer = TfidfVectorizer(analyzer='char',max_features = 6000,ngram_range = (1,6))\n",
    "tfidf_transformer = TfidfVectorizer(max_features = 6000)\n",
    "\n",
    "tfidf_transformer.fit(two_k_data['text'])\n",
    "train_x = tfidf_transformer.transform(X_train)\n",
    "test_x= tfidf_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the labelled data to get best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [0.1, 1, 10]\n",
    "hyperparameters = {'C': c, 'kernel': ['linear']}  \n",
    "SVM = svm.SVC(probability=True)\n",
    "res = GridSearchCV(SVM, hyperparameters)\n",
    "bestmodel = res.fit(train_x,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  82.4074074074074\n",
      "F1 Score:  81.73569023569024\n",
      "Precision:  82.41292434440398\n"
     ]
    }
   ],
   "source": [
    "pred = bestmodel.predict(test_x)\n",
    "\n",
    "print(\"Accuracy: \",accuracy_score( y_test,pred)*100)\n",
    "print(\"F1 Score: \",f1_score(y_test,pred, average=\"weighted\")*100)\n",
    "print(\"Precision: \",precision_score(y_test, pred, average=\"weighted\")*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Preprocessing the 23.9k data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_csv('23900.csv')\n",
    "df2.columns = [\"text\"]\n",
    "df2['text'] = [text_prepare(x) for x in df2['text']]\n",
    "df2['text'] = [entry.lower() for entry in df2['text']]\n",
    "df2['text']= [word_tokenize(entry) for entry in df2['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23900, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['wish']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['goooood', 'girl', 'uh', 'huh', 'please', 'ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['body', 'shaming', 'one', 'cruel', 'unnecessa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['hey', 'glowin', 'presentin', 'new', 'body', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['presentin', 'new', 'body', 'scrubs', 'body',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                                           ['wish']\n",
       "1  ['goooood', 'girl', 'uh', 'huh', 'please', 'ar...\n",
       "2  ['body', 'shaming', 'one', 'cruel', 'unnecessa...\n",
       "3  ['hey', 'glowin', 'presentin', 'new', 'body', ...\n",
       "4  ['presentin', 'new', 'body', 'scrubs', 'body',..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabelled_data  = df2\n",
    "for index,entry in enumerate(unlabelled_data['text']):\n",
    "    Final_words = []\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(entry):\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    unlabelled_data.loc[index,'text'] = str(Final_words)\n",
    "unlabelled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = unlabelled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting labels using already trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_transformer = TfidfVectorizer(analyzer='char',max_features = 6000,ngram_range = (1,6))\n",
    "tfidf_transformer = TfidfVectorizer(max_features = 6000)\n",
    "\n",
    "tfidf_transformer.fit(unlabelled_data['text'])\n",
    "data = tfidf_transformer.transform(unlabelled_data['text'])\n",
    "\n",
    "results_prob = bestmodel.predict_proba(data)\n",
    "results_label = bestmodel.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(results_prob)):\n",
    "#     print(results_prob[i][0],\" \",results_prob[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the selected data with a high degree of certainty ( threshold of 0.8 and 0.95 here ) to the labelled data and removing them from unlabelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict = {'text' : [] , 'label':[] }\n",
    "a=[]\n",
    "for i in range(len(results_prob)):\n",
    "#     print(results_prob[i][1],\" \",results_prob[i][1])\n",
    "    if(results_prob[i][1] >= 0.8):\n",
    "        a.append(i)\n",
    "        ddict['text'].append(unlabelled_data.iloc[i]['text'])\n",
    "        ddict['label'].append(1)\n",
    "    elif(results_prob[i][0] >= 0.95):\n",
    "        a.append(i)\n",
    "        ddict['text'].append(unlabelled_data.iloc[i]['text'])\n",
    "        ddict['label'].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_append = pd.DataFrame(ddict)\n",
    "\n",
    "upd_labelled_data = two_k_data\n",
    "upd_labelled_data = upd_labelled_data.append(to_append)\n",
    "upd_labelled_data = upd_labelled_data.reset_index(drop=True)\n",
    "\n",
    "unlabelled_data=unlabelled_data.drop(unlabelled_data.index[a])\n",
    "unlabelled_data = unlabelled_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of updated labelled data and unlabelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6613, 2)\n",
      "(19446, 1)\n"
     ]
    }
   ],
   "source": [
    "print(upd_labelled_data.shape)\n",
    "print(unlabelled_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After adding data to labelled data,  training the model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(upd_labelled_data['text'],upd_labelled_data['label'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_transformer = TfidfVectorizer(analyzer='char',max_features = 6000,ngram_range = (1,6))\n",
    "tfidf_transformer = TfidfVectorizer(max_features = 6000)\n",
    "\n",
    "tfidf_transformer.fit(upd_labelled_data['text'])\n",
    "train_x = tfidf_transformer.transform(X_train)\n",
    "test_x= tfidf_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [0.1, 1, 10]\n",
    "hyperparameters = {'C': c,'kernel': ['linear']}  \n",
    "SVM = svm.SVC(probability=True)\n",
    "res = GridSearchCV(SVM, hyperparameters)\n",
    "bestmodel = res.fit(train_x,y_train)\n",
    "pred = bestmodel.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  91.83673469387756\n",
      "F1 Score:  90.57205225044179\n",
      "Precision:  91.09693877551021\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \",accuracy_score( y_test,pred)*100)\n",
    "print(\"F1 Score: \",f1_score(y_test,pred, average=\"weighted\")*100)\n",
    "print(\"Precision: \",precision_score(y_test, pred, average=\"weighted\")*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting labels using already trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_transformer = TfidfVectorizer(analyzer='char',max_features = 6000,ngram_range = (1,6))\n",
    "tfidf_transformer = TfidfVectorizer(max_features = 6000)\n",
    "\n",
    "tfidf_transformer.fit(unlabelled_data['text'])\n",
    "data = tfidf_transformer.transform(unlabelled_data['text'])\n",
    "results_prob_2 = bestmodel.predict_proba(data)\n",
    "results_label_2 = bestmodel.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(results_prob_2)):\n",
    "#     print(results_prob_2[i][0],\" \",results_prob_2[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the selected data with a high degree of certainty ( threshold of 0.8 and 0.97 here ) to the labelled data and removing them from unlabelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "ddict = {'text' : [] , 'label':[] }\n",
    "for i in range(len(results_prob_2)):\n",
    "    if(results_prob_2[i][1] >= 0.8):\n",
    "        a.append(i)\n",
    "        ddict['text'].append(unlabelled_data.iloc[i]['text'])\n",
    "        ddict['label'].append(1)\n",
    "    elif(results_prob_2[i][0] >= 0.97):\n",
    "        a.append(i)\n",
    "        ddict['text'].append(unlabelled_data.iloc[i]['text'])\n",
    "        ddict['label'].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_append = pd.DataFrame(ddict)\n",
    "\n",
    "upd_labelled_data_2 = upd_labelled_data\n",
    "upd_labelled_data_2 = upd_labelled_data_2.append(to_append)\n",
    "upd_labelled_data_2 = upd_labelled_data_2.reset_index(drop=True)\n",
    "\n",
    "unlabelled_data = unlabelled_data.drop(unlabelled_data.index[a])\n",
    "unlabelled_data = unlabelled_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of updated labelled data and unlabelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8401\n",
      "17658\n"
     ]
    }
   ],
   "source": [
    "print(len(upd_labelled_data_2))\n",
    "print(len(unlabelled_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After adding data to labelled data,  training the model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(upd_labelled_data_2['text'],upd_labelled_data_2['label'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_transformer = TfidfVectorizer(analyzer='char',max_features = 6000,ngram_range = (1,6))\n",
    "tfidf_transformer = TfidfVectorizer(max_features = 6000)\n",
    "\n",
    "tfidf_transformer.fit(upd_labelled_data_2['text'])\n",
    "train_x = tfidf_transformer.transform(X_train)\n",
    "test_x= tfidf_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [0.1, 1, 10]\n",
    "hyperparameters = {'C': c,'kernel': ['linear']}  \n",
    "SVM = svm.SVC(probability=True)\n",
    "res = GridSearchCV(SVM, hyperparameters)\n",
    "bestmodel = res.fit(train_x,y_train)\n",
    "pred = bestmodel.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  92.62343842950625\n",
      "F1 Score:  91.18084545425656\n",
      "Precision:  91.42122614970874\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \",accuracy_score( y_test,pred)*100)\n",
    "print(\"F1 Score: \",f1_score(y_test,pred, average=\"weighted\")*100)\n",
    "print(\"Precision: \",precision_score(y_test, pred, average=\"weighted\")*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting labels using already trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_transformer = TfidfVectorizer(analyzer='char',max_features = 6000,ngram_range = (1,6))\n",
    "tfidf_transformer = TfidfVectorizer(max_features = 6000)\n",
    "\n",
    "tfidf_transformer.fit(unlabelled_data['text'])\n",
    "data = tfidf_transformer.transform(unlabelled_data['text'])\n",
    "results_prob_3 = bestmodel.predict_proba(data)\n",
    "results_label_3 = bestmodel.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(results_prob_3)):\n",
    "#     print(results_prob_3[i][0],\" \",results_prob_3[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the selected data with a high degree of certainty ( threshold of 0.8 and 0.98 here ) to the labelled data and removing them from unlabelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict = {'text' : [] , 'label':[] }\n",
    "a=[]\n",
    "for i in range(len(results_prob_3)):\n",
    "    if(results_prob_3[i][1] >= 0.8):\n",
    "        a.append(i)\n",
    "        ddict['text'].append(unlabelled_data.iloc[i]['text'])\n",
    "        ddict['label'].append(1)\n",
    "    elif(results_prob_3[i][0] >= 0.98):\n",
    "        a.append(i)\n",
    "        ddict['text'].append(unlabelled_data.iloc[i]['text'])\n",
    "        ddict['label'].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_append = pd.DataFrame(ddict)\n",
    "\n",
    "upd_labelled_data_3 = upd_labelled_data_2\n",
    "upd_labelled_data_3 = upd_labelled_data_3.append(to_append)\n",
    "upd_labelled_data_3 = upd_labelled_data_3.reset_index(drop=True)\n",
    "\n",
    "unlabelled_data=unlabelled_data.drop(unlabelled_data.index[a])\n",
    "unlabelled_data = unlabelled_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of updated labelled data and unlabelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10981\n",
      "15078\n"
     ]
    }
   ],
   "source": [
    "print(len(upd_labelled_data_3))\n",
    "print(len(unlabelled_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After adding data to labelled data,  training the model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(upd_labelled_data_3['text'],upd_labelled_data_3['label'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_transformer = TfidfVectorizer(analyzer='char',max_features = 6000,ngram_range = (1,6))\n",
    "tfidf_transformer = TfidfVectorizer(max_features = 6000)\n",
    "\n",
    "tfidf_transformer.fit(upd_labelled_data_3['text'])\n",
    "train_x = tfidf_transformer.transform(X_train)\n",
    "test_x= tfidf_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [0.1, 1, 10]\n",
    "hyperparameters = {'C': c,'kernel': ['linear']}  \n",
    "SVM = svm.SVC(probability=True)\n",
    "res = GridSearchCV(SVM, hyperparameters)\n",
    "bestmodel = res.fit(train_x,y_train)\n",
    "pred = bestmodel.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  93.94629039599454\n",
      "F1 Score:  92.68173288941033\n",
      "Precision:  93.19939540826478\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \",accuracy_score( y_test,pred)*100)\n",
    "print(\"F1 Score: \",f1_score(y_test,pred, average=\"weighted\")*100)\n",
    "print(\"Precision: \",precision_score(y_test, pred, average=\"weighted\")*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
